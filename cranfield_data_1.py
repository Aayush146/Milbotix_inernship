# -*- coding: utf-8 -*-
"""Cranfield_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OtjTMYyq6bvH1u_prwyxMdgY2TzeB4Ym
"""

# pip install the important libraries for this 
# note, if you are using google colab to run the code, you will require the mpld3 file to work interacitvely work with plots

!pip install hampel
!pip install heartpy
!pip install mpld3

# import all the libraries and the python files
# The ampd algorithms ae in this code, so you do not need to create a seperate python file for them, but ensure all these libraries
# are imported 
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt 
import numpy as np
import scipy
from scipy import signal, stats
from scipy.ndimage import uniform_filter1d
from scipy.signal import detrend
os.chdir('/content/drive/MyDrive/Milbotix Internship /Milbotix internship/Cranfield data')
import python_file_3
from python_file_3 import filter_signal, get_hrv, get_rmssd, get_std, heart_rate
import heartpy as hp
import mpld3
from mpld3 import plugins

# find peaks algorithms 
# the research paper is also referenced in the report 

def find_peaks_original(x, scale=None, debug=False):
    """Find peaks in quasi-periodic noisy signals using AMPD algorithm.
    Automatic Multi-Scale Peak Detection originally proposed in
    "An Efficient Algorithm for Automatic Peak Detection in
    Noisy Periodic and Quasi-Periodic Signals", Algorithms 2012, 5, 588-603
    https://doi.org/10.1109/ICRERA.2016.7884365
    Optimized implementation by Igor Gotlibovych, 2018
    Parameters
    ----------
    x : ndarray
        1-D array on which to find peaks
    scale : int, optional
        specify maximum scale window size of (2 * scale + 1)
    debug : bool, optional
        if set to True, return the Local Scalogram Matrix, `LSM`,
        and scale with most local maxima, `l`,
        together with peak locations
    Returns
    -------
    pks: ndarray
        The ordered array of peak indices found in `x`
    """
    x = detrend(x)
    N = len(x)
    L = N // 2
    if scale:
        L = min(scale, L)

    # create LSM matix
    LSM = np.zeros((L, N), dtype=bool)
    for k in np.arange(1, L):
        LSM[k - 1, k:N - k] = (
            (x[0:N - 2 * k] < x[k:N - k]) & (x[k:N - k] > x[2 * k:N])
        )

    # Find scale with most maxima
    G = LSM.sum(axis=1)
    l_scale = np.argmax(G)

    # find peaks that persist on all scales up to l
    pks_logical = np.min(LSM[0:l_scale, :], axis=0)
    pks = np.flatnonzero(pks_logical)
    if debug:
        return pks, LSM, l_scale
    return pks


def find_peaks(x, scale=None, debug=False):
    """Find peaks in quasi-periodic noisy signals using AMPD algorithm.
    Extended implementation handles peaks near start/end of the signal.
    Optimized implementation by Igor Gotlibovych, 2018
    Parameters
    ----------
    x : ndarray
        1-D array on which to find peaks
    scale : int, optional
        specify maximum scale window size of (2 * scale + 1)
    debug : bool, optional
        if set to True, return the Local Scalogram Matrix, `LSM`,
        weigted number of maxima, 'G',
        and scale at which G is maximized, `l`,
        together with peak locations
    Returns
    -------
    pks: ndarray
        The ordered array of peak indices found in `x`
    """
    x = detrend(x)
    N = len(x)
    L = N // 2
    if scale:
        L = min(scale, L)

    # create LSM matix
    LSM = np.ones((L, N), dtype=bool)
    for k in np.arange(1, L + 1):
        LSM[k - 1, 0:N - k] &= (x[0:N - k] > x[k:N]
                                )  # compare to right neighbours
        LSM[k - 1, k:N] &= (x[k:N] > x[0:N - k])  # compare to left neighbours

    # Find scale with most maxima
    G = LSM.sum(axis=1)
    G = G * np.arange(
        N // 2, N // 2 - L, -1
    )  # normalize to adjust for new edge regions
    l_scale = np.argmax(G)

    # find peaks that persist on all scales up to l
    pks_logical = np.min(LSM[0:l_scale, :], axis=0)
    pks = np.flatnonzero(pks_logical)
    if debug:
        return pks, LSM, G, l_scale
    return pks


def find_peaks_adaptive(x, window=None, debug=False):
    """Find peaks in quasi-periodic noisy signals using ASS-AMPD algorithm.
    Adaptive Scale Selection Automatic Multi-Scale Peak Detection,
    an extension of AMPD -
    "An Efficient Algorithm for Automatic Peak Detection in
    Noisy Periodic and Quasi-Periodic Signals", Algorithms 2012, 5, 588-603
    https://doi.org/10.1109/ICRERA.2016.7884365
    Optimized implementation by Igor Gotlibovych, 2018
    Parameters
    ----------
    x : ndarray
        1-D array on which to find peaks
    window : int, optional
        sliding window size for adaptive scale selection
    debug : bool, optional
        if set to True, return the Local Scalogram Matrix, `LSM`,
        and `adaptive_scale`,
        together with peak locations
    Returns
    -------
    pks: ndarray
        The ordered array of peak indices found in `x`
    """
    x = detrend(x)
    N = len(x)
    if not window:
        window = N
    if window > N:
        window = N
    L = window // 2

    # create LSM matix
    LSM = np.ones((L, N), dtype=bool)
    for k in np.arange(1, L + 1):
        LSM[k - 1, 0:N - k] &= (x[0:N - k] > x[k:N]
                                )  # compare to right neighbours
        LSM[k - 1, k:N] &= (x[k:N] > x[0:N - k])  # compare to left neighbours

    # Create continuos adaptive LSM
    ass_LSM = uniform_filter1d(LSM * window, window, axis=1, mode='nearest')
    normalization = np.arange(L, 0, -1)  # scale normalization weight
    ass_LSM = ass_LSM * normalization.reshape(-1, 1)

    # Find adaptive scale at each point
    adaptive_scale = ass_LSM.argmax(axis=0)

    # construct reduced LSM
    LSM_reduced = LSM[:adaptive_scale.max(), :]
    mask = (np.indices(LSM_reduced.shape)[0] > adaptive_scale
            )  # these elements are outside scale of interest
    LSM_reduced[mask] = 1

    # find peaks that persist on all scales up to l
    pks_logical = np.min(LSM_reduced, axis=0)
    pks = np.flatnonzero(pks_logical)
    if debug:
        return pks, ass_LSM, adaptive_scale
    return pks

''' the code outputs the average heart rate for each of the section (i.e five minute time interval)'''
''' however, to change the type of peak detection algorithm, you will need to manually do that'''
''' at the moment, it is set to the find peaks original algorithm'''

def get_heart_rate(data, sampling_rate):

  section_list = []
  
  times = np.arange(0,len(data))/64

  splits = dict([(5,300), (10,600), (15,900), (20,1200), (25,1500)])
  splits_2 = dict([(1,60), (2,120), (3,180), (4,240)])
                  
  for i in splits:
    if i == 5:
      index_point = np.asarray(np.where(times == splits[5]))[0][0]
      signa = out_signal[:index_point]
      section_list.append(signa)
    elif i == 10:
      index_point_1 = np.asarray(np.where(times == splits[5]))[0][0]
      index_point_2 = np.asarray(np.where(times == splits[10]))[0][0]
      signa_a = out_signal[index_point_1:index_point_2]
      section_list.append(signa_a)
    elif i == 15:
      index_point_1 = np.asarray(np.where(times == splits[10]))[0][0]
      index_point_2 = np.asarray(np.where(times == splits[15]))[0][0]
      signa_b = out_signal[index_point_1:index_point_2]
      section_list.append(signa_b)
    elif i == 20:
      index_point_1 = np.asarray(np.where(times == splits[15]))[0][0]
      index_point_2 = np.asarray(np.where(times == splits[20]))[0][0]
      signa_c = out_signal[index_point_1:index_point_2]
      section_list.append(signa_c)
    elif i == 25:
      index_point_1 = np.asarray(np.where(times == splits[20]))[0][0]
      index_point_2 = np.asarray(np.where(times == splits[25]))[0][0]
      signa_d = out_signal[index_point_1:index_point_2]
      section_list.append(signa_d)
    elif i == 30:
      index_point_1 = np.asarray(np.where(times == splits[25]))[0][0]
      index_point_2 = np.asarray(np.where(times == splits[30]))[0][0] 
      signa_e = out_signal[index_point_1:index_point_2]
      section_list.append(signa_e)
    else:
      continue
  
  for i in range(len(section_list)):
    avg_heart_rate = []
    time = np.arange(len(section_list[i]))/64
    for j in splits_2:
       index = np.asarray(np.where(time == splits_2.get(j)))
       if j == 1:
         analysis_signal = section_list[i][:index[0][0]]
         time_2 = np.arange(len(analysis_signal))/64
         Y_2 = np.fft.rfft(analysis_signal)
         f_3 = np.arange(0,len(Y_2))*64/len(analysis_signal)
         max_freq = np.where(Y_2 == max(Y_2))
         signal_peaks = find_peaks_original(analysis_signal) 

         for c in range(len(signal_peaks)):
           number_of_points = []
           if signal_peaks[c] >= np.where(time_2 == 10):
             break
           else:
             number_of_points.append(signal_peaks[i])
             heart_rate_t = len(number_of_points)/10
             heart_rate_t = heart_rate_t * 60
             heart_rate_f = f_3[max_freq] * 60
             heart_rates = np.asarray([heart_rate_f, heart_rate_t])
             mean_rate = np.mean(heart_rates)
             avg_heart_rate.append(mean_rate)
       else:
          index_1 = np.asarray(np.where(time == splits_2.get(j-1)))
          index_2 = np.asarray(np.where(time == splits_2.get(j)))
          analysis_signal = section_list[i][index_1[0][0]:index_2[0][0]]
          time_2 = np.arange(len(analysis_signal))/64
          Y_2 = np.fft.rfft(analysis_signal)
          f_3 = np.arange(0,len(Y_2))*64/len(analysis_signal)
          max_freq = np.where(Y_2 == max(Y_2))
          signal_peaks = find_peaks_original(analysis_signal) 
          for c in range(len(signal_peaks)):
            number_of_points = []
            if signal_peaks[c] >= np.where(time_2 == 10):
              break
            else:
              number_of_points.append(signal_peaks[i])
              heart_rate_t = len(number_of_points)/10
              heart_rate_t = heart_rate_t * 60
              heart_rate_f = f_3[max_freq] * 60
              heart_rates = np.asarray([heart_rate_f, heart_rate_t])
              mean_rate = np.mean(heart_rates)
              avg_heart_rate.append(mean_rate)

    avg_rate = sum(avg_heart_rate)/len(avg_heart_rate)
    print('the average heart rate for section {} is {}'.format(i,avg_rate))

"""## Heart rate extraction """

'''load the ppg sensor data as a CSV file, it is the best way to wokr with the following code, otherwise a test tile would do as well. '''
''' It is important to note that this code is for google colab, so ensure that the data files and this python notebook are in the same direcotry'''

path = '/content/drive/MyDrive/Milbotix Internship /Milbotix internship/Cranfield data/s2p23.csv'
data_1 = pd.read_csv(path)
data_1.rename(columns = {'PPG Last (V)':'Volts (V)'},
              inplace = True)
signal_a = data_1['Volts (V)']
signal_a_1 = signal_a - np.mean(signal_a)
signal_a_1 = signal_a_1.dropna()
times = np.arange(0,len(signal_a_1))/64

''' filter the signal using the filter_signal function, this ic bespoke to the ppg sensor data'''
out_signal = filter_signal(signals =signal_a_1, sampling_fs = 64, 
                            f_order = 4, low_cut = 0.7, high_cut = 2.5,
                            kernel_size = 7)

plt.figure(figsize = (20,8))
plt.plot(times, out_signal)
plt.title('s2p23 dataset signal filtered', fontsize = 25)
plt.xlabel('Time (s)', fontsize = 20)
mpld3.display()

get_heart_rate(out_signal, sampling_rate = 64)

"""## heart rate variability"""

'''load the ppg sensor data as a CSV file, it is the best way to wokr with the following code, otherwise a test tile would do as well. '''
''' It is important to note that this code is for google colab, so ensure that the data files and this python notebook are in the same direcotry'''

path = '/content/drive/MyDrive/Milbotix Internship /Milbotix internship/Cranfield data/s2p23.csv'
data_1 = pd.read_csv(path)
data_1.rename(columns = {'PPG Last (V)':'Volts (V)'},
              inplace = True)
signal_a = data_1['Volts (V)']
signal_a_1 = signal_a - np.mean(signal_a)
signal_a_1 = signal_a_1.dropna()
times = np.arange(0,len(signal_a_1))/64

''' filter the signal using the filter_signal function, this ic bespoke to the ppg sensor data'''
out_signal = filter_signal(signals =signal_a_1, sampling_fs = 64, 
                            f_order = 4, low_cut = 0.7, high_cut = 2.5,
                            kernel_size = 7)

'''to analyse a  data file with 5 minute intervals, the data is split into 7 different sections( depending on the size of the data)'''
''' a dictionary has been used'''
section_list = []
splits = dict([(5,300), (10,600), (15,900), (20,1200), (25,1500)])
for i in splits:
  if i == 5:
    index_point = np.asarray(np.where(times == splits[5]))[0][0]
    signa = out_signal[:index_point]
    section_list.append(signa)
  elif i == 10:
    index_point_1 = np.asarray(np.where(times == splits[5]))[0][0]
    index_point_2 = np.asarray(np.where(times == splits[10]))[0][0]
    signa_a = out_signal[index_point_1:index_point_2]
    section_list.append(signa_a)
  elif i == 15:
    index_point_1 = np.asarray(np.where(times == splits[10]))[0][0]
    index_point_2 = np.asarray(np.where(times == splits[15]))[0][0]
    signa_b = out_signal[index_point_1:index_point_2]
    section_list.append(signa_b)
  elif i == 20:
    index_point_1 = np.asarray(np.where(times == splits[15]))[0][0]
    index_point_2 = np.asarray(np.where(times == splits[20]))[0][0]
    signa_c = out_signal[index_point_1:index_point_2]
    section_list.append(signa_c)
  elif i == 25:
    index_point_1 = np.asarray(np.where(times == splits[20]))[0][0]
    index_point_2 = np.asarray(np.where(times == splits[25]))[0][0]
    signa_d = out_signal[index_point_1:index_point_2]
    section_list.append(signa_d)
  elif i == 30:
    index_point_1 = np.asarray(np.where(times == splits[25]))[0][0]
    index_point_2 = np.asarray(np.where(times == splits[30]))[0][0] 
    signa_e = out_signal[index_point_1:index_point_2]
    section_list.append(signa_e)
  else:
    continue

''' here the find_peaks algorithm is used to detect all the peaks in a 5 minute unterval'''
''' Nite- you will have to manually select a section of the data to apply the find peaks,
however, it can very easily be automated'''

sig = find_peaks_original(section_4[0]) # detect the peak's index points
t_5 = np.arange(0, len(section_4[0]))/64 # obtain the time length of the signal
tp = []
for i in times[sig]:
    tp.append(i)
difference = [t-s for s,t in zip(tp, tp[1:])]  # extract the successive intervals between each of the peaks detected using a loop function

dif_rms = get_rmssd(difference) # import the get_rmssd function and apply it
std_1 = get_std(difference) # import the get_std function and apply it

